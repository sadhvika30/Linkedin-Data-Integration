source.class=com.linkedin.cdi.source.S3SourceV2
fs.uri=file://localhost/
state.store.fs.uri=file://localhost/
converter.classes=org.apache.gobblin.converter.avro.JsonIntermediateToAvroConverter
data.publisher.final.dir=/tmp/test-lms-s3/output
dataset.urn=com.linkedin.lms-s3-upload.fileupload
extract.table.name=fileupload
extract.namespace=com.linkedin.test
extract.table.type=SNAPSHOT_ONLY
extract.is.full=true
job.description=upload files
job.name=testJob
ms.extractor.class=com.linkedin.cdi.extractor.JsonExtractor
ms.secondary.input=[{"path": "/tmp/test-lms-s3/testInput/testFile", "category": "payload", "format":"binary"}]
ms.source.uri=https://<bucketName>.s3.amazonaws.com/test-lms-s3
ms.work.unit.parallelism.max=300
taskexecutor.threadpool.size=20
writer.builder.class=org.apache.gobblin.writer.AvroDataWriterBuilder
writer.file.path=testFileUpload
writer.include.record.count.in.file.names=true
writer.destination.type=HDFS
writer.file.path.type=namespace_table
writer.output.format=AVRO
source.conn.username=
source.conn.password=
ms.source.s3.parameters={"region" : "us-west-1"}
ms.output.schema=[{"columnName":"eTag","isNullable":"false","dataType":{"type":"string"}}]
encrypt.key.loc=/tmp/test-lms-s3/master-key/master.key